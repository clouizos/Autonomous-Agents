\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods and Procedures}{2}}
\newlabel{methods}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\textbf  {Action selection}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\textbf  {On-Policy Monte Carlo}}{3}}
\newlabel{returnOn}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces On-policy Monte Carlo Control pseudo-code}}{4}}
\newlabel{onmc}{{2.1}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\textbf  {Off-Policy Monte Carlo}}{4}}
\newlabel{eq:relative prob 1}{{2.4}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Off-Policy Monte Carlo Control Pseudo-code}}{5}}
\newlabel{figure:offPolicy}{{2.2}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\textbf  {Sarsa}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sarsa Pseudo-code}}{6}}
\newlabel{figure:Sarsa}{{2.3}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}\textbf  {Q-learning}}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Q-learning: An off-policy TD control algorithm.}}{7}}
\newlabel{figure:Q}{{2.4}{7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{7}}
\newlabel{results}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Theoretical Differences}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q-learning with $\epsilon $-greedy: experiments on $\alpha $ and $\gamma $}{8}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Experimental setup for testing $\alpha $}}{8}}
\newlabel{expSetupAlpha}{{3.1}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Convergence overview for different values of $\alpha $.}}{9}}
\newlabel{figure:alphaOverview}{{3.1}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convergence for different values of $\alpha $ in the initial 2000 runs.}}{9}}
\newlabel{figure:alphafirst20}{{3.2}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Convergence for different values of $\alpha $ in the last 2000 runs.}}{10}}
\newlabel{figure:alphalast20}{{3.3}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Optimal policy in the reduced state space from Value Iteration given the prey at (5,5).}}{10}}
\newlabel{table:optimalPolicy}{{3.2}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Optimality for different values of $\alpha $ as a ratio to the optimal policy.}}{11}}
\newlabel{figure:alphaOpti}{{3.4}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Experimental setup for testing $\gamma $}}{12}}
\newlabel{expSetupGamma}{{3.3}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Convergence overview for different values of $\gamma $.}}{12}}
\newlabel{figure:gammaOverview}{{3.5}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Convergence for different values of $\gamma $ in the initial 2000 runs.}}{13}}
\newlabel{figure:gammafirst20}{{3.6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Convergence for different values of $\gamma $ in the last 2000 runs.}}{13}}
\newlabel{figure:gammalast20}{{3.7}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Optimality for different values of $\gamma $ as a ratio to the optimal policy.}}{14}}
\newlabel{figure:gammaOpti}{{3.8}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Optimality with the interaction of $\alpha $ and $\gamma $.}}{14}}
\newlabel{figure:alphagammaOpti}{{3.9}{14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Q-learning with $\epsilon $-greedy: experiments on $\epsilon $ and optimistic initialization}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Experimental setup for testing $\epsilon $}}{15}}
\newlabel{expSetupEpsilon}{{3.4}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Convergence overview for different values of $\epsilon $.}}{16}}
\newlabel{figure:epsilonOverview}{{3.10}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Convergence for different values of $\epsilon $ in the initial 2000 runs.}}{16}}
\newlabel{figure:epsilonfirst20}{{3.11}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Convergence for different values of $\epsilon $ in the last 2000 runs.}}{17}}
\newlabel{figure:epsilonlast20}{{3.12}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Optimality for different values of $\epsilon $ as a ratio to the optimal policy}}{17}}
\newlabel{figure:epsilonOpti}{{3.13}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Experimental setup for testing optimistic initialization}}{18}}
\newlabel{expSetupOptimInit}{{3.5}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Values from value iteration when the prey is at [5][5]}}{18}}
\newlabel{table:vi}{{3.6}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Convergence and divergence overview for variable optimistic initialization.}}{19}}
\newlabel{figure:optimisticOverview}{{3.14}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Convergence and divergence for variable optimistic initialization in the initial 2000 runs.}}{19}}
\newlabel{figure:optimisticfirst20}{{3.15}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence for variable optimistic initialization in the last 2000 runs.}}{20}}
\newlabel{figure:optimisticlast20}{{3.16}{20}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Divergence from the optimal policy after optimal initialization in case of VI.}}{20}}
\newlabel{figure:optiOpti}{{3.17}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Q-learning with Softmax action selection: experiments on $\tau $ and comparison with $\epsilon $-greedy}{21}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Experimental setup for testing $\tau $}}{21}}
\newlabel{expSetupSoftmax}{{3.7}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Convergence overview for different values of $\tau $ in comparison to $\epsilon $-greedy.}}{22}}
\newlabel{figure:softmaxOverview}{{3.18}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Convergence for different values of $\tau $ in the initial 2000 runs in comparison to $\epsilon $-greedy.}}{22}}
\newlabel{figure:softmaxfirst20}{{3.19}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Convergence for different values of $\tau $ in the last 2000 runs in comparison to $\epsilon $-greedy.}}{23}}
\newlabel{figure:softmaxlast20}{{3.20}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Optimality for different values of $\tau $ as a ratio to the optimal policy in comparison to $\epsilon $-greedy.}}{23}}
\newlabel{figure:softmxOpti}{{3.21}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Deterioration from the optimal policy for different values of $\tau $ in comparison to $\epsilon $-greedy.}}{24}}
\newlabel{figure:softmxGrace}{{3.22}{24}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Sarsa: a comparison with Q-learning in $\epsilon $-greedy and Softmax action selection}{24}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces Experimental setup for Sarsa and Q-learning comparisons.}}{25}}
\newlabel{expSetupSarsa}{{3.8}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Convergence overview for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection.}}{25}}
\newlabel{figure:sarsaOverview}{{3.23}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Convergence for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection in the initial 2000 runs.}}{26}}
\newlabel{figure:sarsafirst20}{{3.24}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces Convergence for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection in the last 2000 runs.}}{26}}
\newlabel{figure:sarsalast20}{{3.25}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces Optimality for for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection as a ratio to the optimal policy.}}{27}}
\newlabel{figure:sarsaOpti}{{3.26}{27}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}On-Policy Monte Carlo Control}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces On Policy MC catching time with different epsilon value}}{28}}
\newlabel{figure:differentFixEpsilonOn}{{3.27}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces On Policy MC catching time with optimistic initial Q value}}{29}}
\newlabel{figure:differentFixEpsilonOnOpt}{{3.28}{29}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces Target Policy Off-Policy Monte Carlo Control}}{30}}
\newlabel{table:onpolicy}{{3.9}{30}}
\@writefile{lot}{\contentsline {table}{\numberline {3.10}{\ignorespaces Optimal state value function for each state given prey at (5,5)}}{30}}
\newlabel{table:valueFunctionOn}{{3.10}{30}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Off-Policy Monte Carlo Control}{30}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces Catching time with fix epsilon value}}{31}}
\newlabel{figure:fixEpsilon}{{3.29}{31}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces Catching time with dynamic epsilon value}}{32}}
\newlabel{figure:dynamicEpsilon}{{3.30}{32}}
\newlabel{figure:dynamicEpsilon}{{3.30}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces Catching time with different fixed epsilon value}}{33}}
\newlabel{figure:differentFixEpsilon}{{3.31}{33}}
\@writefile{lot}{\contentsline {table}{\numberline {3.11}{\ignorespaces Optimal state value function for each state given prey at (5,5)}}{34}}
\newlabel{table:valueFunctionOff}{{3.11}{34}}
\@writefile{lot}{\contentsline {table}{\numberline {3.12}{\ignorespaces Optimal target policy's actions for each state given the prey at (5,5)}}{34}}
\newlabel{table:TargetPolicyOff}{{3.12}{34}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Comparison of all approaches to the learning problem}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces Catching times for best choice of $\epsilon $ for On and Off policy MC}}{35}}
\newlabel{figure:onoffcomp}{{3.32}{35}}
\@writefile{lot}{\contentsline {table}{\numberline {3.13}{\ignorespaces Average catching time for all algorithms at convergence}}{35}}
\newlabel{compavg}{{3.13}{35}}
\@writefile{toc}{\contentsline {section}{\numberline {4}conclusion}{35}}
\newlabel{discussion}{{4}{35}}
\bibcite{suttonBarto}{1}
