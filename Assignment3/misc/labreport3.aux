\relax 
\select@language{english}
\@writefile{toc}{\select@language{english}}
\@writefile{lof}{\select@language{english}}
\@writefile{lot}{\select@language{english}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods and Procedures}{2}}
\newlabel{methods}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}\textbf  {Action selection}}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}\textbf  {On-Policy Monte Carlo}}{2}}
\newlabel{returnOn}{{2.2}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces On-policy Monte Carlo Control pseudo-code}}{3}}
\newlabel{onmc}{{2.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}\textbf  {Off-Policy Monte Carlo}}{3}}
\newlabel{eq:relative prob 1}{{2.4}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Off-Policy Monte Carlo Control Pseudo-code}}{4}}
\newlabel{figure:offPolicy}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}\textbf  {Sarsa}}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Sarsa Pseudo-code}}{5}}
\newlabel{figure:Sarsa}{{2.3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}\textbf  {Q-learning}}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces Q-learning: An off-policy TD control algorithm.}}{5}}
\newlabel{figure:Q}{{2.4}{5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experiments and Results}{5}}
\newlabel{results}{{3}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Theoretical Differences}{5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Q-learning with $\epsilon $-greedy: experiments on $\alpha $ and $\gamma $}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces Experimental setup for testing $\alpha $}}{6}}
\newlabel{expSetupAlpha}{{3.1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Convergence overview for different values of $\alpha $.}}{6}}
\newlabel{figure:alphaOverview}{{3.1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces Convergence for different values of $\alpha $ in the initial 2000 runs.}}{6}}
\newlabel{figure:alphafirst20}{{3.2}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces Convergence for different values of $\alpha $ in the last 2000 runs.}}{7}}
\newlabel{figure:alphalast20}{{3.3}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces Optimal policy in the reduced state space from Value Iteration given the prey at position (5,5).}}{7}}
\newlabel{table:optimalPolicy}{{3.2}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces Optimality for different values of $\alpha $ as a ratio to the optimal policy.}}{7}}
\newlabel{figure:alphaOpti}{{3.4}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {3.3}{\ignorespaces Experimental setup for testing $\gamma $}}{8}}
\newlabel{expSetupGamma}{{3.3}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces Convergence overview for different values of $\gamma $.}}{8}}
\newlabel{figure:gammaOverview}{{3.5}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Convergence for different values of $\gamma $ in the initial 2000 runs.}}{8}}
\newlabel{figure:gammafirst20}{{3.6}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Convergence for different values of $\gamma $ in the last 2000 runs.}}{8}}
\newlabel{figure:gammalast20}{{3.7}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces Optimality for different values of $\gamma $ as a ratio to the optimal policy.}}{8}}
\newlabel{figure:gammaOpti}{{3.8}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces Optimality with the interaction of $\alpha $ and $\gamma $.}}{8}}
\newlabel{figure:alphagammaOpti}{{3.9}{8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Q-learning with $\epsilon $-greedy: experiments on $\epsilon $ and optimistic initialization}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {3.4}{\ignorespaces Experimental setup for testing $\epsilon $}}{9}}
\newlabel{expSetupEpsilon}{{3.4}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.10}{\ignorespaces Convergence overview for different values of $\epsilon $.}}{9}}
\newlabel{figure:epsilonOverview}{{3.10}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.11}{\ignorespaces Convergence for different values of $\epsilon $ in the initial 2000 runs.}}{9}}
\newlabel{figure:epsilonfirst20}{{3.11}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.12}{\ignorespaces Convergence for different values of $\epsilon $ in the last 2000 runs.}}{9}}
\newlabel{figure:epsilonlast20}{{3.12}{9}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.13}{\ignorespaces Optimality for different values of $\epsilon $ as a ratio to the optimal policy}}{9}}
\newlabel{figure:epsilonOpti}{{3.13}{9}}
\@writefile{lot}{\contentsline {table}{\numberline {3.5}{\ignorespaces Experimental setup for testing optimistic initialization}}{10}}
\newlabel{expSetupOptimInit}{{3.5}{10}}
\@writefile{lot}{\contentsline {table}{\numberline {3.6}{\ignorespaces Values from value iteration when the prey is positioned at [5][5]}}{10}}
\newlabel{table:vi}{{3.6}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.14}{\ignorespaces Convergence and divergence overview for variable optimistic initialization.}}{10}}
\newlabel{figure:optimisticOverview}{{3.14}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.15}{\ignorespaces Convergence and divergence for variable optimistic initialization in the initial 2000 runs.}}{10}}
\newlabel{figure:optimisticfirst20}{{3.15}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.16}{\ignorespaces Convergence for variable optimistic initialization in the last 2000 runs.}}{10}}
\newlabel{figure:optimisticlast20}{{3.16}{10}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.17}{\ignorespaces Divergence from the optimal policy after optimal initialization in case of VI.}}{10}}
\newlabel{figure:optiOpti}{{3.17}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Q-learning with Softmax action selection: experiments on $\tau $ and comparison with $\epsilon $-greedy}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3.7}{\ignorespaces Experimental setup for testing $\tau $}}{11}}
\newlabel{expSetupSoftmax}{{3.7}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.18}{\ignorespaces Convergence overview for different values of $\tau $ in comparison to $\epsilon $-greedy.}}{11}}
\newlabel{figure:softmaxOverview}{{3.18}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.19}{\ignorespaces Convergence for different values of $\tau $ in the initial 2000 runs in comparison to $\epsilon $-greedy.}}{11}}
\newlabel{figure:softmaxfirst20}{{3.19}{11}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.20}{\ignorespaces Convergence for different values of $\tau $ in the last 2000 runs in comparison to $\epsilon $-greedy.}}{12}}
\newlabel{figure:softmaxlast20}{{3.20}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.21}{\ignorespaces Optimality for different values of $\tau $ as a ratio to the optimal policy in comparison to $\epsilon $-greedy.}}{12}}
\newlabel{figure:softmxOpti}{{3.21}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.22}{\ignorespaces Deterioration from the optimal policy for different values of $\tau $ in comparison to $\epsilon $-greedy.}}{12}}
\newlabel{figure:softmxGrace}{{3.22}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Sarsa: a comparison with Q-learning in $\epsilon $-greedy and Softmax action selection}{12}}
\@writefile{lot}{\contentsline {table}{\numberline {3.8}{\ignorespaces Experimental setup for Sarsa and Q-learning comparisons.}}{12}}
\newlabel{expSetupSarsa}{{3.8}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.23}{\ignorespaces Convergence overview for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection.}}{13}}
\newlabel{figure:sarsaOverview}{{3.23}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.24}{\ignorespaces Convergence for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection in the initial 2000 runs.}}{13}}
\newlabel{figure:sarsafirst20}{{3.24}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.25}{\ignorespaces Convergence for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection in the last 2000 runs.}}{13}}
\newlabel{figure:sarsalast20}{{3.25}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.26}{\ignorespaces Optimality for for Sarsa and Q-learning in $\epsilon $-greedy and Softmax action selection as a ratio to the optimal policy.}}{13}}
\newlabel{figure:sarsaOpti}{{3.26}{13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}On-Policy Monte Carlo Control}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.27}{\ignorespaces On Policy MC catching time with different epsilon value}}{14}}
\newlabel{figure:differentFixEpsilonOn}{{3.27}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.28}{\ignorespaces On Policy MC catching time with optimistic initial Q value}}{14}}
\newlabel{figure:differentFixEpsilonOnOpt}{{3.28}{14}}
\@writefile{lot}{\contentsline {table}{\numberline {3.9}{\ignorespaces Target Policy Off-Policy Monte Carlo Control}}{15}}
\newlabel{table:onpolicy}{{3.9}{15}}
\@writefile{lot}{\contentsline {table}{\numberline {3.10}{\ignorespaces Optimal state value function for each state given prey at (5,5)}}{15}}
\newlabel{table:valueFunctionOn}{{3.10}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Off-Policy Monte Carlo Control}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.29}{\ignorespaces Catching time with fix epsilon value}}{16}}
\newlabel{figure:fixEpsilon}{{3.29}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.30}{\ignorespaces Catching time with dynamic epsilon value}}{16}}
\newlabel{figure:dynamicEpsilon}{{3.30}{16}}
\newlabel{figure:dynamicEpsilon}{{3.30}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.31}{\ignorespaces Catching time with different fixed epsilon value}}{17}}
\newlabel{figure:differentFixEpsilon}{{3.31}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.11}{\ignorespaces Optimal state value function for each state given prey at (5,5)}}{17}}
\newlabel{table:valueFunctionOff}{{3.11}{17}}
\@writefile{lot}{\contentsline {table}{\numberline {3.12}{\ignorespaces Optimal target policy's actions for each state given the prey at (5,5)}}{18}}
\newlabel{table:TargetPolicyOff}{{3.12}{18}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Comparison of all approaches to the learning problem}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.32}{\ignorespaces Catching times for best choice of $\epsilon $ for On and Off policy MC}}{18}}
\newlabel{figure:onoffcomp}{{3.32}{18}}
\@writefile{lot}{\contentsline {table}{\numberline {3.13}{\ignorespaces Average catching time for all algorithms at convergence}}{18}}
\newlabel{compavg}{{3.13}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {4}conclusion}{18}}
\newlabel{discussion}{{4}{18}}
\bibcite{suttonBarto}{1}
